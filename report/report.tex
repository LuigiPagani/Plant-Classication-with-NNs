\documentclass[conference,compsoc,11pt]{IEEEtran} %11 pt ok


\usepackage{mathptmx} % Times New Roman-like font ok

\usepackage{multirow}

\usepackage[style=verbose]{biblatex}
\addbibresource{./bibliography.bib}

\ifCLASSINFOpdf
\else
\fi

\usepackage{graphicx}

\graphicspath{ {../images/} }

\begin{document}
\title{Convolutional Neural Network:\\ Healthy \emph{vs} Unhealthy Plants Classification
\\ \large Artificial Neural Networks and Deep Learning - A.Y. 2023/2024}

\author{\IEEEauthorblockN{Luigi Pagani\IEEEauthorrefmark{1},
Flavia Petruso\IEEEauthorrefmark{2} and Federico Schermi\IEEEauthorrefmark{3}}
\IEEEauthorblockA{M.Sc. Mathematical Engineering,
Politecnico di Milano - Milan, Italy\\
Email: \IEEEauthorrefmark{1}luigi.pagani@mail.polimi.it,
\IEEEauthorrefmark{2}federico.schermi@mail.polimi.it,
\IEEEauthorrefmark{3}flavia.petruso@mail.polimi.it\\
Student ID: \IEEEauthorrefmark{1}10677832,
\IEEEauthorrefmark{2}10544566,
\IEEEauthorrefmark{3}10776811\\
Codalab Groupname: ``RandNet''}
}

%--------------------------------------------------------------------------------------
\vspace{-0.3cm}

\maketitle


\IEEEpeerreviewmaketitle

%--------------------------------------------------------------------------------------
\section{Introduction and dataset description}
\vspace{-0.3cm}

The provided dataset is composed by NUMERO INIZIALE images of plants (leaves), each one represented as a 96*96*3 tensor, with the first two dimensions indicating the pixel size and the third the rgb channels. Data are (unequally) divided into two classes, namely XXX \emph{healthy} and XXX \emph{unhealthy} plants, and the project goal is to train a classifier which is able to discriminate between the two \\


%%--------------------------------------------------------------------------------------
\section {Preprocessing}
\vspace{-0.3cm}
The first step we carried on was data inspection and cleaning. This led to the identification and removal of two types of outliers, for an overall number of INSERIRE NUMERO outliers. 'sto downsizing to float 16???. We then performed a train-validation split of 80-20. 


%--------------------------------------------------------------------------------------
\section {Data augmentation}
\vspace{-0.3cm}

To increase the numerosity of the data and improve generalizability of our results we performed data augmentation on the training set CONTROLLARE. After a brief study of the most reasonable transformations to apply to our data, we opted for a mix of basic transformations (including shifts, shear, rotation, flip, and brightness) within a visually reasonable range, and decided to not perfoorm zoom augmentation, to avoid the risks of losing areas which could help discriminate the two types of plants. 

%--------------------------------------------------------------------------------------
\section{CNN}

\vspace{-0.3cm}

To deal with the problem of class imbalance, we decided to assign weights to the classes, in a way which was inversely proportional to the frequency of each class in the dataset.


\subsection{Feature extraction layers}
\vspace{-0.2cm}

To determine the structure of the Feature extraction layer, after tryin some basic structures from scratch we decided to opt for a transfer learning approach. To choose the network with the best performance, we benchmarked several networks, namely ConvNetXLarge, EfficientNetLarge and .... At first, as our main focus was the performance, we tried to use a a single classification layer with 512 neurons followed by a dropout layer of 0.3. From the validation accuracy it appears that


\subsection{Classification layers}
\vspace{-0.2cm}


We used keras tuner...

As an output layer we used a single neuron with sigmoid activation, due to the binary nature of the classification task.


%--------------------------------------------------------------------------------------
\section {Model regularization strategies}
\vspace{-0.3cm}

To avoid the risk of overfitting, we employed different regularization strategies. \\ First, we performed data augmentation as previously explained. Then, we decided to use the Dropout after each dense layer. The dropout level was set ....... (even supported by the results of our analysis performed with keras tuner). Finally, we introduced L1-L2 regularization to introduce a b√¨as in the loss function and reduce the varinace. 

\section {Model parameters and metrics}
\vspace{-0.3cm}

Due to the binary nature of the classification tasks, we use the binary crossentropy a loss metric. 

%--------------------------------------------------------------------------------------
\section {Test time augmentation}
\vspace{-0.3cm}

After the model was chosen, we relied on test-time augmentation as a strategy to improve the test performance. We used ....... After some trial and error efforts, we used ... as a batch size. The augmentation allowed for an increase of model accuracy on the final model of ....  
%--------------------------------------------------------------------------------------


\section {Results and performance} 
\vspace{-0.3cm}


%--------------------------------------------------------------------------------------
\section{References}
\vspace{-0.3cm}

Da capire se metterle o no

%--------------------------------------------------------------------------------------
\section{Contributions}
\vspace{-0.3cm}
\begin {itemize}
	\item {\textbf{Data inspection and preprocessing}}: Luigi Pagani, Flavia Petruso, Federico Schermi
	\item  {\textbf{Model development}}: Luigi Pagani, Flavia Petruso, Federico Schermi
	\item  {\textbf{Report writing}}: Luigi Pagani, Flavia Petruso, Federico Schermi
	\item  {\textbf{GAM}}: Luigi Pagani, Federico Schermi (?)
\end {itemize}



\end{document}